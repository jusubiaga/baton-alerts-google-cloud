"use strict";
// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     https://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
Object.defineProperty(exports, "__esModule", { value: true });
exports.JSONEncoder = void 0;
const protobuf = require("protobufjs");
const protos = require("../../protos/protos");
const proto_1 = require("../adapt/proto");
const extend = require("extend");
const DescriptorProto = protos.google.protobuf.DescriptorProto;
const { Type } = protobuf;
/**
 * Internal class used by the JSONWriter to convert JSON data to protobuf messages.
 * It can be configure to do some data conversion to match what BigQuery expects.
 *
 * @class
 * @memberof managedwriter
 */
class JSONEncoder {
    /**
     * Creates a new JSONEncoder instance.
     *
     * @param {Object} params - The parameters for the JSONEncoder.
     * @param {IDescriptorProto} params.protoDescriptor - The proto descriptor
     *   for the JSON rows.
     */
    constructor(params) {
        this._type = Type.fromJSON('root', {
            fields: {},
        });
        const { protoDescriptor } = params;
        this.setProtoDescriptor(protoDescriptor);
    }
    /**
     * Update the proto descriptor for the Encoder.
     *
     * @param {IDescriptorProto} protoDescriptor - The proto descriptor.
     */
    setProtoDescriptor(protoDescriptor) {
        const normalized = (0, proto_1.normalizeDescriptor)(new DescriptorProto(protoDescriptor));
        this._type = Type.fromDescriptor(normalized);
    }
    /**
     * Writes a JSONList that contains objects to be written to the BigQuery table by first converting
     * the JSON data to protobuf messages, then using Writer's appendRows() to write the data at current end
     * of stream. If there is a schema update, the current Writer is closed and reopened with the updated schema.
     *
     * @param {JSONList} rows - The list of JSON rows.
     * @returns {Uint8Array[]} The encoded rows.
     */
    encodeRows(rows) {
        const serializedRows = rows
            .map(r => {
            return this.convertRow(r, this._type);
        })
            .map(r => {
            return this.encodeRow(r);
        });
        return serializedRows;
    }
    isPlainObject(value) {
        return value && [undefined, Object].includes(value.constructor);
    }
    encodeRow(row) {
        const msg = this._type.create(row);
        return this._type.encode(msg).finish();
    }
    convertRow(source, ptype) {
        const row = extend(true, {}, source);
        for (const key in row) {
            const value = row[key];
            if (value === null) {
                continue;
            }
            const encodedValue = this.encodeRowValue(value, key, ptype);
            if (encodedValue === undefined) {
                continue;
            }
            row[key] = encodedValue;
        }
        return row;
    }
    encodeRowValue(value, key, ptype) {
        const pfield = ptype.fields[key];
        if (!pfield) {
            return undefined;
        }
        if (value instanceof Date) {
            switch (pfield.type) {
                case 'int32': // DATE
                    // The value is the number of days since the Unix epoch (1970-01-01)
                    return value.getTime() / (1000 * 60 * 60 * 24);
                case 'int64': // TIMESTAMP
                    // The value is given in microseconds since the Unix epoch (1970-01-01)
                    return value.getTime() * 1000;
                case 'string': // DATETIME
                    return value.toJSON().replace(/^(.*)T(.*)Z$/, '$1 $2');
            }
            return undefined;
        }
        // NUMERIC and BIGNUMERIC integer
        if (typeof value === 'number' || typeof value === 'bigint') {
            switch (pfield.type) {
                case 'string':
                    return value.toString(10);
            }
            return undefined;
        }
        if (Array.isArray(value)) {
            const subType = this.getSubType(key, ptype);
            return value.map(v => {
                if (this.isPlainObject(v)) {
                    return this.convertRow(v, subType);
                }
                const encodedValue = this.encodeRowValue(v, key, subType);
                if (encodedValue === undefined) {
                    return v;
                }
                return encodedValue;
            });
        }
        if (this.isPlainObject(value)) {
            const subType = this.getSubType(key, ptype);
            return this.convertRow(value, subType);
        }
        return undefined;
    }
    getSubType(key, ptype) {
        const pfield = ptype.fields[key];
        if (!pfield) {
            return ptype;
        }
        try {
            const subType = ptype.lookupTypeOrEnum(pfield.type);
            return subType;
        }
        catch (err) {
            return ptype;
        }
    }
}
exports.JSONEncoder = JSONEncoder;
//# sourceMappingURL=encoder.js.map