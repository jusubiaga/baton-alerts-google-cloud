import * as protos from '../../protos/protos';
import { JSONObject } from './json_writer';
type IDescriptorProto = protos.google.protobuf.IDescriptorProto;
/**
 * Internal class used by the JSONWriter to convert JSON data to protobuf messages.
 * It can be configure to do some data conversion to match what BigQuery expects.
 *
 * @class
 * @memberof managedwriter
 */
export declare class JSONEncoder {
    private _type;
    /**
     * Creates a new JSONEncoder instance.
     *
     * @param {Object} params - The parameters for the JSONEncoder.
     * @param {IDescriptorProto} params.protoDescriptor - The proto descriptor
     *   for the JSON rows.
     */
    constructor(params: {
        protoDescriptor: IDescriptorProto;
    });
    /**
     * Update the proto descriptor for the Encoder.
     *
     * @param {IDescriptorProto} protoDescriptor - The proto descriptor.
     */
    setProtoDescriptor(protoDescriptor: IDescriptorProto): void;
    /**
     * Writes a JSONList that contains objects to be written to the BigQuery table by first converting
     * the JSON data to protobuf messages, then using Writer's appendRows() to write the data at current end
     * of stream. If there is a schema update, the current Writer is closed and reopened with the updated schema.
     *
     * @param {JSONList} rows - The list of JSON rows.
     * @returns {Uint8Array[]} The encoded rows.
     */
    encodeRows(rows: JSONObject[]): Uint8Array[];
    private isPlainObject;
    private encodeRow;
    private convertRow;
    private encodeRowValue;
    private getSubType;
}
export {};
